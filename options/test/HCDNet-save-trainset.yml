# ------------------------------------------------------------------------
# Copyright (c) 2022 megvii-model. All Rights Reserved.
# ------------------------------------------------------------------------
# Modified from BasicSR (https://github.com/xinntao/BasicSR)
# Copyright 2018-2020 BasicSR Authors
# ------------------------------------------------------------------------
# general settings
name: HCDNet
model_type: HCDNet_with_RSBlur_model
scale: 1
num_gpu: 1
manual_seed: 42

save_HCDNet_for_training: true

datasets:
  val:
    name: HCBlur-train
    type: HCBlur_with_K_dataset
    dataroot_gt: ./datasets/HCBlur_Syn_train
    dataroot_lq: ./datasets/HCBlur_Syn_train
    dataroot_flows: ./datasets/HCBlur_Syn_train/shortUW_flows
    dataroot_uws: ./datasets/HCBlur_Syn_train/shortUW
    io_backend:
      type: disk

    # new params
    datalist: datalist/HCBlur_Syn_train.txt

    use_K: false
    use_interpolated_K: true
    normalize_K: true

    RSBlur: true

network_g:
  type: HCDNet_Local
  width: 16
  enc_blk_nums: [1, 1, 1, 1]
  middle_blk_num: 1
  dec_blk_nums: [1, 1, 1, 1]
  train_size: [1, 3, 384, 384]

# path
path:
  pretrain_network_g: 'pretrained_models/HC-DNet.pth'
  strict_load_g: true
  resume_state: ~
  deblur_path: './datasets/HCBlur_Syn_train_HCDNet'

# training settings
train:
  optim_g:
    type: AdamW
    lr: !!float 1e-3
    weight_decay: !!float 1e-3
    betas: [0.9, 0.9]

  scheduler:
    type: TrueCosineAnnealingLR
    T_max: 300000
    eta_min: !!float 1e-7

  total_iter: 300000
  warmup_iter: -1 # no warm up

  # losses
  pixel_opt:
    type: PSNRLoss
    loss_weight: 1
    reduction: mean

# validation settings
val:
  val_freq: 30000 #!!float 2e4
  save_img: false

  metrics:
    psnr: # metric name, can be arbitrary
      type: calculate_psnr
      crop_border: 0
      test_y_channel: false
    ssim:
      type: calculate_ssim
      crop_border: 0
      test_y_channel: false

# logging settings
logger:
  print_freq: 20 #200
  save_checkpoint_freq: !!float 5e3
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# dist training settings
dist_params:
  backend: nccl
  port: 29500
